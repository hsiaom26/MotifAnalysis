{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A implementation of RasMMA\n",
    "### (Runtime API call sequence-based Motif Mining Algorithm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "% run Alignment_Fast3.ipynb\n",
    "\n",
    "# Doing global alignment and find commonAPISequence.\n",
    "def do_globalAlignment(rep1, rep2):\n",
    "    # Aligment\n",
    "    commonAPISequence = []\n",
    "    alignment_result = globalAlign( rep1, rep2, 10, -1, 0)[2]\n",
    "    common_motif_sequence = motif_delimit(alignment_result)\n",
    "    return common_motif_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def removeDuplicateAPI(featureTrace): # remove duplicate api if continuously occur\n",
    "    result = []\n",
    "    lastAPI = \"\"\n",
    "    for api in featureTrace:\n",
    "        if lastAPI != api: # find new api\n",
    "            result.append(api)\n",
    "            lastAPI = api\n",
    "    return result\n",
    "\n",
    "def removeUnwantedAPI(featureTrace): # remove unwanted api\n",
    "    result = []\n",
    "    unwanted_api = {'CloseHandle', 'OpenThread', 'RegOpenKey', 'RegCloseKey'}\n",
    "    frequently_used_lib = {'imm32', 'lpk', 'gdi32', 'kernel32', 'ntdll', 'user32', 'comctl32', 'advapi64'}\n",
    "\n",
    "    for api in featureTrace:\n",
    "        API = api.split('#')[0]\n",
    "        \n",
    "        if API == \"LoadLibrary\": # api is LoadLibrary\n",
    "            libName = api.split(\"@\")[2]\n",
    "            if libName not in frequently_used_lib: # found new library, add it into lib_set and result_Hooklog\n",
    "                result.append(api)\n",
    "                frequently_used_lib.update(libName)\n",
    "                \n",
    "        elif API not in unwanted_api: # api not unwanted\n",
    "            result.append(api)\n",
    "            \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "% run FeatureTrace.ipynb\n",
    "\n",
    "# initialize all traces as \"to merge candidates clusters\"\n",
    "def initialCandidateDict(data_directory):\n",
    "    \n",
    "#     toMergeCandidate_List = list()\n",
    "    toMergeCandidate_Dict = dict()\n",
    "    \n",
    "    # get feature hooklogs\n",
    "    FeatTrace = FeatureTrace\n",
    "    traceName_list = list(filter(lambda f:f.endswith('.trace.hooklog'), os.listdir(data_directory))) # trace Name List\n",
    "    ft_count = 0\n",
    "    for traceName in traceName_list:\n",
    "        featureTrace = FeatTrace(data_directory + traceName).getTrace_noContainTS()\n",
    "#         featureTrace = [line.rstrip('\\n') for line in open(data_directory + traceName)] # use txt as featureTrace directly\n",
    "        featureTrace = removeDuplicateAPI(featureTrace)    \n",
    "        featureTrace = removeUnwantedAPI(featureTrace)\n",
    "        clusterName = \"G\"+str(ft_count)\n",
    "        # R = tuple( clusterName, list of common motif Sequence ), the representative of cluster.\n",
    "        R = (clusterName, [featureTrace])\n",
    "        clusterMembers = set()\n",
    "        traceName = shortenHooklogName(traceName)\n",
    "        clusterMembers.add(traceName)\n",
    "        \n",
    "        toMergeCandidate_Dict[ft_count] = (R, clusterMembers)\n",
    "        \n",
    "        ft_count+=1\n",
    "        \n",
    "    print(\"-- Finish Initializing --\")\n",
    "    return toMergeCandidate_Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# shorten Name to first 6 charactors\n",
    "def shortenHooklogName(traceName):\n",
    "    hashValue = traceName[0:6]\n",
    "    pid = traceName.split(\"_\")[1].split(\".\")[0]\n",
    "    return hashValue+\"_\"+pid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# input: two R\n",
    "# output: new Rep's common motif sequence of input CMS;\n",
    "def get_Rep_CommMotifSeq(Ri, Rj):\n",
    "    rep1 = []\n",
    "    for motif in Ri[1]:\n",
    "        rep1.extend(motif)\n",
    "    rep2 = []\n",
    "    for motif in Rj[1]:\n",
    "        rep2.extend(motif)\n",
    "    repNew_CMS = []\n",
    "    if(rep1 and rep2):\n",
    "        commonSequence = do_globalAlignment(rep1, rep2) # do Alignment\n",
    "        repNew = commonSequence\n",
    "    return repNew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# return a dictionary that contains the initializing informations\n",
    "#\n",
    "# initialDict = {clusterName : (originalName, initialLength)}\n",
    "\n",
    "def getInitialDict(toMergeCandidateDict):\n",
    "    initialDict = dict()\n",
    "    for key, value in toMergeCandidateDict.items():\n",
    "        clusterName = value[0][0]\n",
    "        comm_motif_Seq = value[0][1]\n",
    "        repAPISeq = []\n",
    "        for motif in comm_motif_Seq:\n",
    "            repAPISeq.extend(motif)\n",
    "        initialLen = len(repAPISeq)\n",
    "        originalName = value[1].pop()\n",
    "        value[1].add(originalName)\n",
    "        initialDict[clusterName] = (originalName, initialLen)\n",
    "    return initialDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# return a dict that contains only original name\n",
    "# nameDict = {clusterName: original name}\n",
    "\n",
    "def getInitialNameDict(initialDict):\n",
    "    nameDict = dict()\n",
    "    for key, value in initialDict.items():\n",
    "        name = value[0]\n",
    "        nameDict[key] = name\n",
    "    return nameDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# compute score of Rnew\n",
    "# the score calculate method is the length ratio of new to origin one\n",
    "\n",
    "# Ri is a tuple like ('G0', [[['A#A', 'C#C'], 0, 1, (0, 1), (1, 2)]])\n",
    "def compute_Score(Ri, Rj, Rnew):\n",
    "    if(Rnew[1]):\n",
    "        repi = []\n",
    "        for API_motif in Ri[1]:\n",
    "            repi.extend(API_motif)\n",
    "        repj = []\n",
    "        for API_motif in Rj[1]:\n",
    "            repj.extend(API_motif)\n",
    "        repNew = []\n",
    "        for API_motif in Rnew[1]:\n",
    "            repNew.extend(API_motif)\n",
    "        L_Ri = len(repi)\n",
    "        L_Rj = len(repj)\n",
    "        Lorg = max(L_Ri, L_Rj)\n",
    "        Lnew = len(repNew)\n",
    "        return float(Lnew)/Lorg\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get score list of toMergeCandidateDict(single iteration) from highest to lowest\n",
    "\n",
    "def findMergeCandidateScoreList(toMergeCandidateDict, generatedSeqNum):\n",
    "    scoreList = list()\n",
    "    dictKeys = list(toMergeCandidateDict.keys())\n",
    "    \n",
    "    sensitiveAPIs = {\"CreateProcessInternal\", \"OpenProcess\", \"WinExec\", \"CreateThread\", \"OpenThread\", \"CreateRemoteThread\",\n",
    "                     \"CopyFile\", \"CreateFile\", \"WriteFile\", \"ReadFile\", \"DeleteFile\", \"RegCreateKey\", \"RegSetValue\",\n",
    "                     \"InternetOpen\", \"InternetConnect\", \"HttpSendRequest\", \"WinHttpOpen\", \"WinHttpSendRequest\", \"WinHttpWriteData\", \"WinHttpCreateUrl\"}\n",
    "    \n",
    "    for i in range(len(dictKeys)):\n",
    "        for j in range(i+1, len(dictKeys)):\n",
    "            \n",
    "            # toMergeCandidateDict[i][1] is memberSet\n",
    "            Ri = toMergeCandidateDict[ dictKeys[i] ][0] # Ri is a tuple like ('G0', [['A#A', 'C#C'], ['MMM']])\n",
    "            Rj = toMergeCandidateDict[ dictKeys[j] ][0]\n",
    "            repNew_CMS = get_Rep_CommMotifSeq(Ri, Rj) # get rep's common motif seq.\n",
    "            clusterTempName = \"G\" + str(generatedSeqNum)\n",
    "            Rnew = (clusterTempName, repNew_CMS)\n",
    "            score = compute_Score(Ri, Rj, Rnew)\n",
    "            Ri_name = Ri[0]\n",
    "            Rj_name = Rj[0]\n",
    "            scoreList.append((score, Rnew, Ri_name, Rj_name))\n",
    "\n",
    "    if(len(scoreList) > 0):\n",
    "        scoreList.sort(key=lambda tup:tup[0], reverse=True) # sorting by score (from biggest to smallest) \n",
    "        print(\"ScoreList Length in method : \", len(scoreList))\n",
    "    else:\n",
    "        print(\"No any merge candidate\")\n",
    "    \n",
    "    return scoreList # list = [(score, Rnew, Ri_name, Rj_name), (score, Rnew, Ri_name, Rj_name), ...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def checkExactlySameCandidates(scoreList):\n",
    "    globalPoolDict = dict() # a dict contains many sets.  dict = {index0: memberSet, 1: memberSet, 2:...}\n",
    "    newScoreList = list() # list = [(score, R, memberSet), (score, R, memberSet), ...]\n",
    "    scoreListIdx = 0\n",
    "    for rank in scoreList:\n",
    "        score = rank[0]\n",
    "        \n",
    "        if(score == 1.0):\n",
    "            \n",
    "            Ri_name = rank[2]\n",
    "            Rj_name = rank[3]\n",
    "            \n",
    "            duplicate = False\n",
    "            for key, memberSet in globalPoolDict.items():\n",
    "                if(Ri_name in memberSet) or (Rj_name in memberSet):\n",
    "                    memberSet.add(Ri_name)\n",
    "                    memberSet.add(Rj_name)\n",
    "                    \n",
    "                    # update newScoreList 'memberSet' element\n",
    "                    newScoreList[key] = (newScoreList[key][0], newScoreList[key][1], memberSet)\n",
    "                    duplicate = True\n",
    "                    \n",
    "            # Find new independent pair, add into newScoreList and create new dict key\n",
    "            if(duplicate is False):\n",
    "                memberSet = set()\n",
    "                memberSet.add(Ri_name)\n",
    "                memberSet.add(Rj_name)\n",
    "                globalPoolDict[scoreListIdx] = memberSet\n",
    "                \n",
    "                Rnew = rank[1]\n",
    "                newScoreList.append((score, Rnew, memberSet))\n",
    "                scoreListIdx += 1\n",
    "        else:\n",
    "            Rnew = rank[1]\n",
    "            Ri_name = rank[2]\n",
    "            Rj_name = rank[3]\n",
    "            memberSet = set()\n",
    "            memberSet.add(Ri_name)\n",
    "            memberSet.add(Rj_name)\n",
    "            newScoreList.append((score, Rnew, memberSet))\n",
    "            scoreListIdx += 1\n",
    "    globalPoolDict = None\n",
    "    return newScoreList # list = [(score, R, memberSet), (score, R, memberSet), ...]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # unit test\n",
    "# item1 = (1.0, (\"G0\", \"[['A#A', 'B#B','B#B', 'C#C','D#D'], 0, 2]\"), \"a.txt\", \"b.txt\")\n",
    "# item2 = (1.0, (\"G1\", \"[['A#A', 'B#B','B#B', 'C#C','D#D'], 0, 2]\"), \"a.txt\", \"c.txt\")\n",
    "# item3 = (1.0, (\"G2\", \"[['A#A', 'B#B','B#B', 'C#C','D#D'], 0, 2]\"), \"b.txt\", \"c.txt\")\n",
    "# item4 = (1.0, (\"G3\", \"[['A#A', 'B#B','B#B', 'C#C','D#D'], 0, 2]\"), \"c.txt\", \"d.txt\")\n",
    "# item5 = (1.0, (\"G4\", \"[['E#A', 'F#B'], 0, 2]\"), \"e.txt\", \"f.txt\")\n",
    "# item6 = (0.8, (\"G5\", \"[['X#A', 'Y#B'], 0, 2]\"), \"x.txt\", \"y.txt\")\n",
    "\n",
    "# scoreList = [item1, item2, item3, item4, item5, item6]\n",
    "\n",
    "# newScoreList = checkExactlySameCandidates(scoreList)\n",
    "# print(newScoreList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# add Rnew into toMergeCandidateDict and remove member of Rnew from candidates.\n",
    "\n",
    "def mergeCandidateClusters(toMergeCandidateDict, intermediatePoolDict, scoreList, generatedSeqNum, initialDict, definedThreshold):\n",
    "    initialNameDict = getInitialNameDict(initialDict) # get original name for reference in output.\n",
    "    \n",
    "    currentMergedSet = set()\n",
    "    for rank in scoreList:\n",
    "        score = rank[0]\n",
    "        memberSet = rank[2] # memberSet of highest score\n",
    "\n",
    "        # the minmum score this round is smaller than threshold\n",
    "        if(score < definedThreshold):\n",
    "            break\n",
    "        \n",
    "        exclusiveness = False\n",
    "        \n",
    "        # check exclusiveness\n",
    "        for member in memberSet:\n",
    "            if(member in currentMergedSet):\n",
    "                exclusiveness = True\n",
    "                break\n",
    "                \n",
    "        if(not exclusiveness):\n",
    "            clusterMembers = set() # create cluster member set with original Name\n",
    "            for member in memberSet:\n",
    "                nameOfMember = int(member.split('G')[1])\n",
    "                del toMergeCandidateDict[nameOfMember]\n",
    "                \n",
    "                if member in initialNameDict:\n",
    "                    clusterMembers.add(initialNameDict[member])\n",
    "                else:\n",
    "                    clusterMembers.add(member)\n",
    "                    \n",
    "                # Mark elements are merged\n",
    "                currentMergedSet.add(member) # update currentMergedSet\n",
    "            \n",
    "            Rnew = rank[1][1] # representative without old clusterName (i.e., rank[1] = (Name, Rep.))\n",
    "            newName = \"G\" + str(generatedSeqNum)\n",
    "            new_Cluster = (newName, Rnew)\n",
    "            \n",
    "            toMergeCandidateDict[generatedSeqNum] = (new_Cluster, clusterMembers)\n",
    "            intermediatePoolDict[generatedSeqNum] = (score, new_Cluster, clusterMembers) # (score, newCluster, members)\n",
    "            generatedSeqNum += 1\n",
    "    currentMergedSet = None\n",
    "    return toMergeCandidateDict, intermediatePoolDict, generatedSeqNum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "### Main Function of RasMMA ###\n",
    "import pickle\n",
    "\n",
    "def do_RasMMA_clustering(data_directory, tag, outputPath, thresholdValue):\n",
    "    \n",
    "    testDict = {0: (('G0', [['A#A', 'B#B','B#B', 'C#C','D#D']]),{\"a.trace.hooklog\"}),\n",
    "                1:(('G1', [['A#A','B#B','C#C','D#D',\"G#G\"]]),{\"b.trace.hooklog\"}),\n",
    "                   2:(('G2', [[\"B#B\",'F#F','C#C','D#D', 'G#G']]),{\"c.trace.hooklog\"}),\n",
    "                      3:(('G3', [['Q#Q','C#C','D#D','G#G','M#M']]),{\"d.trace.hooklog\"}),\n",
    "                           4:(('G4', [['A#A','Q#Q','C#C','G#G','M#M']]),{\"e.trace.hooklog\"})}\n",
    "\n",
    "    intermediatePool = dict()\n",
    "    roundInfos = dict()\n",
    "    residual = None # used to save residual candidate when algorithm stop.\n",
    "#     toMergeCandidateDict = testDict\n",
    "    toMergeCandidateDict = initialCandidateDict(data_directory) # initialize @toMergeCandidateDict\n",
    "#     print(toMergeCandidateDict)\n",
    "    # initialDict = {clusterName : (originalName, initialLength)}\n",
    "    initialDict = getInitialDict(toMergeCandidateDict)\n",
    "    \n",
    "    roundProduct = list()\n",
    "    for key, value in initialDict.items():\n",
    "        roundProduct.append(key)\n",
    "    roundInfos[0] = roundProduct # record product in round 0 (i.e., initialization)\n",
    "    \n",
    "    generatedSeqNum = len(toMergeCandidateDict) # counter after initialize. Used to naming clusters.\n",
    "\n",
    "    print(\"-- Start Clustering --\")\n",
    "    print(\"Threshold set =\", thresholdValue)\n",
    "    roundCounter = 1\n",
    "#     generatedSeqNum = 206\n",
    "#     roundCounter = 2\n",
    "    if(roundCounter != 1):\n",
    "        with open(outputPath+ \"pickle/\" + 'toMergeCandidate_round'+str(roundCounter)+'.pickle', 'rb') as mHandle:\n",
    "            toMergeCandidateDict = pickle.load(mHandle)\n",
    "        with open(outputPath+ \"pickle/\" + 'roundInfos_round'+str(roundCounter-1)+'.pickle', 'rb') as rHandle:\n",
    "            roundInfos = pickle.load(rHandle)\n",
    "        with open(outputPath+ \"pickle/\" + 'intermediate_round'+str(roundCounter-1)+'.pickle', 'rb') as iHandle:\n",
    "            intermediatePool = pickle.load(iHandle)\n",
    "    \n",
    "    while(1):\n",
    "        print(\"Round: \", roundCounter)\n",
    "        if(len(toMergeCandidateDict) == 1):\n",
    "            residual = toMergeCandidateDict # output residual candidates.\n",
    "            break\n",
    "\n",
    "        # calculate scoreList in candidate clusters\n",
    "        scoreList = findMergeCandidateScoreList(toMergeCandidateDict, generatedSeqNum)\n",
    "        \n",
    "        # check and merge exactly the same candidates before merge clusters\n",
    "        scoreList = checkExactlySameCandidates(scoreList)\n",
    "\n",
    "        \n",
    "        # generated Clusters in This Round:\n",
    "        nameIdxStart = generatedSeqNum\n",
    "        \n",
    "        toMergeCandidateDict, intermediatePool, generatedSeqNum = mergeCandidateClusters(\n",
    "            toMergeCandidateDict, intermediatePool, scoreList, generatedSeqNum, initialDict, thresholdValue)\n",
    "        \n",
    "        print(\"generatedSeqNum now: \", generatedSeqNum)\n",
    "        \n",
    "        # check if algorithm should stop when merge score under threshold\n",
    "        # if a score smaller than threshold, then it will break out when merging.\n",
    "        # Hense, if the 'generatedSeqNum' equals than 'nameIdxStart', means that no any new generated cluster.\n",
    "        # (if occurr a new cluster, generatedSeqNum will add one.)\n",
    "        if(generatedSeqNum == nameIdxStart):\n",
    "            residual = toMergeCandidateDict # output residual candidates.\n",
    "            break # end algorithm\n",
    "        \n",
    "        nameIdxEnd = generatedSeqNum\n",
    "        \n",
    "        # Record clusters generated in this round\n",
    "        for idx in range(nameIdxStart, nameIdxEnd):\n",
    "            if roundInfos.get(roundCounter) is None:\n",
    "                roundProduct = list()\n",
    "                roundProduct.append(intermediatePool[idx][1][0])\n",
    "                roundInfos[roundCounter] = roundProduct\n",
    "            else:\n",
    "                roundInfos[roundCounter].append(intermediatePool[idx][1][0])\n",
    "                \n",
    "        roundCounter += 1\n",
    "        \n",
    "        with open(outputPath+ \"pickle/\" + 'toMergeCandidate_round'+str(roundCounter)+'.pickle', 'wb') as mHandle:\n",
    "            pickle.dump(toMergeCandidateDict, mHandle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        with open(outputPath+ \"pickle/\" + 'intermediate_round'+str(roundCounter-1)+'.pickle', 'wb') as iHandle:\n",
    "            pickle.dump(intermediatePool, iHandle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        with open(outputPath+ \"pickle/\" + 'roundInfos_round'+str(roundCounter-1)+'.pickle', 'wb') as rHandle:\n",
    "            pickle.dump(roundInfos, rHandle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "            \n",
    "    print(\"-- Finish Clustering --\")\n",
    "    \n",
    "    for pkl in os.listdir(outputPath+'pickle/'):\n",
    "        if pkl.startswith('toMergeCandidate_round'):\n",
    "            os.remove(outputPath+'pickle/'+pkl)\n",
    "        elif pkl.startswith('intermediate_round'):\n",
    "            os.remove(outputPath+'pickle/'+pkl)\n",
    "        elif pkl.startswith('roundInfos_round'):\n",
    "            os.remove(outputPath+'pickle/'+pkl)\n",
    "            \n",
    "    print(\"-- Clean Temp Pickle Files --\")\n",
    "\n",
    "    return intermediatePool, initialDict, roundInfos, residual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clusterInitializedReps(initializedReps_dict, tag, outputPath, thresholdValue):\n",
    "    intermediatePool = dict()\n",
    "    roundInfos = dict()\n",
    "    residual = None # used to save residual candidate when algorithm stop.\n",
    "#     toMergeCandidateDict = testDict\n",
    "    toMergeCandidateDict = initializedReps_dict # using residualRepsDict as toMergeCandidateDict (skip initialization)\n",
    "\n",
    "    # initialDict = {clusterName : (originalName, initialLength)}\n",
    "    initialDict = getInitialDict(toMergeCandidateDict)\n",
    "    \n",
    "    roundProduct = list()\n",
    "    for key, value in initialDict.items():\n",
    "        roundProduct.append(key)\n",
    "    roundInfos[0] = roundProduct # record product in round 0 (i.e., initialization)\n",
    "    \n",
    "    generatedSeqNum = len(toMergeCandidateDict) # counter after initialize. Used to naming clusters.\n",
    "\n",
    "    print(\"-- Start Clustering --\")\n",
    "    print(\"Threshold set =\", thresholdValue)\n",
    "    roundCounter = 1\n",
    "    \n",
    "    while(1):\n",
    "        print(\"Current Round : Round \", roundCounter)\n",
    "        if(len(toMergeCandidateDict) == 1):\n",
    "            residual = toMergeCandidateDict # output residual candidates.\n",
    "            break\n",
    "\n",
    "        # calculate scoreList in candidate clusters\n",
    "        scoreList = findMergeCandidateScoreList(toMergeCandidateDict, generatedSeqNum)\n",
    "        print(\"-- Finish scoring --\")\n",
    "        print(\"ScoreList Len : \", len(scoreList))\n",
    "        \n",
    "        # check and merge exactly the same candidates before merge clusters\n",
    "        scoreList = checkExactlySameCandidates(scoreList)\n",
    "        print(\"-- Finish checking 100% same candidates --\")\n",
    "             \n",
    "        # generated Clusters in This Round:\n",
    "        nameIdxStart = generatedSeqNum\n",
    "        \n",
    "        toMergeCandidateDict, intermediatePool, generatedSeqNum = mergeCandidateClusters(\n",
    "            toMergeCandidateDict, intermediatePool, scoreList, generatedSeqNum, initialDict, thresholdValue)\n",
    "        print(\"-- Finish merging clusters --\")\n",
    "        # check if algorithm should stop when merge score under threshold\n",
    "        # if a score smaller than threshold, then it will break out when merging.\n",
    "        # Hense, if the 'generatedSeqNum' equals than 'nameIdxStart', means that no any new generated cluster.\n",
    "        # (if occurr a new cluster, generatedSeqNum will add one.)\n",
    "        if(generatedSeqNum == nameIdxStart):\n",
    "            residual = toMergeCandidateDict # output residual candidates.\n",
    "            break # end algorithm\n",
    "        \n",
    "        nameIdxEnd = generatedSeqNum\n",
    "        \n",
    "        # Record clusters generated in this round\n",
    "        for idx in range(nameIdxStart, nameIdxEnd):\n",
    "            if roundInfos.get(roundCounter) is None:\n",
    "                roundProduct = list()\n",
    "                roundProduct.append(intermediatePool[idx][1][0])\n",
    "                roundInfos[roundCounter] = roundProduct\n",
    "            else:\n",
    "                roundInfos[roundCounter].append(intermediatePool[idx][1][0])\n",
    "                \n",
    "        roundCounter += 1\n",
    "    print(\"-- Finish Clustering --\")\n",
    "\n",
    "    return intermediatePool, initialDict, roundInfos, residual"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
